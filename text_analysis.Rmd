---
title: "Text Analysis"
author: "Jinyu Li"
date: "2021/12/06"
output: 
  pdf_document


editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=F,message = F,echo=F,highlight=F)
#knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="png",fig.align  = 'center')
knitr::opts_chunk$set(fig.width=5, fig.height=5,fig.align = "center") 
pacman::p_load(
tidyverse,
magrittr,
knitr,
gutenbergr,
tidytext,
sentimentr,
textdata
)



```

## Task 1: Pick a Book

The book I chose is written by Charlse Dickens, and the book name is
"Hard Times" and shortened as "A Christmas Carol" because we are gonna
spend our final time of the year at Christmas.

```{r}
# search the books from one author
# gutenberg_works(str_detect(author, "Dickens"))

# get the book from the website
Christmas_df = gutenberg_download(gutenberg_id = 46)

#my_books=hard_times_df

# save it into txt  
# write.table(Christmas_df,'A_Christmas_Carol.txt',row.names = F)

```

```{r}
library(tnum)
tnum.authorize("mssp1.bu.edu")
tnum.setSpace("test2")
source("Book2TN-v6A-1.R")

```

```{r}
# get the table from txt
Christmas_table <-read.table('A_Christmas_Carol.txt',header = T)

```

## TASK 2: Bag of Word Analysis

In this part, I will show the sentiment analysis by using AFINN, Bing
and NRC respectively. I am going to plot several barplots to compare
these 3 methods And show the differences of them.

The book, A Christmas Carol, is in general a book with more negative
sentiments than positive sentiments.

To briefly summarize the book, at the very beginning, the book describes
the background of Ireland in the 1840s, where people were suffering from
hungers and coldness. The leading character of the novel is Scrooge, who
is a scrooge literally. He loves money and does not spare his mercy to
others. On Christmas eve, so many ghosts visited Scrooge's house and
made him see his death. After the night, Scrooge realized that money
would be gone one day and changed himself and

In conclusion, according to the plotline, the sentiment of the book
should be negetive at first and positive at the very last, which I find
Bing lexicon and Afinn lexicon both work well. It's hard to tell which
one works the better. The only difference bewteen these 2 I find is that
the sentiment by Bing lexicon is more negetive, which I think may fit
the book better.

Consequently, in the following part, I will mainly do the sentiment
analysis by using Bing lexicon.

```{r}
# generally display the book saved in dataframe
# head(Christmas_table)
```

```{r}
# Find the chapter for the book;
# Add one column to record the chapter number for each token

tidy_Christmas <- Christmas_table %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("<")
                                      ))) %>%
  unnest_tokens(word, text)

#tidy_Christmas$chapter%>%unique()
#tidy_Christmas%>%view()
```

### The difference in the sentiment analysis with different lexicons

I now use 3 lexicons to get the sentiment analysis for every word in the
book, and try to tell the difference among the 3 methods

The first one is Afinn lexicon(outcome is 1945rows X 5columns):

```{r }
# do the inner join to get the sentiment analysis by afinn lexicon for each word in the book
afinn_Christmas_list <- tidy_Christmas %>% 
  inner_join(get_sentiments("afinn"))

kable(head(afinn_Christmas_list), caption = "Word-level analysis -- Afinn lexicon")
kable(unique(afinn_Christmas_list$value), col.names = "sentiment", caption = "Value column -- Afinn lexicon")
structure(afinn_Christmas_list)
```

The second one is Bing lexicon(outcome is 1926rows X 6columns):

```{r}
# do the inner join to get the sentiment analysis by bing lexicon for each word in the book
bing_Christmas_list <- tidy_Christmas %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al.")

kable(head(bing_Christmas_list),caption = "Word-level analysis -- Bing lexicon")
kable(unique(bing_Christmas_list$sentiment), col.names = "sentiment", caption = "Sentiment column -- Bing lexicon")
structure(bing_Christmas_list)
```

The third one is nrc lexicon(outcome is 6203rows X 6columns):

```{r}
# do the inner join to get the sentiment analysis by nrc lexicon for each word in the book
nrc_Christmas_list <- tidy_Christmas %>% 
    inner_join(get_sentiments("nrc")) %>% 
                 # filter(sentiment %in% c("positive", 
                 #                         "negative")))%>%
    mutate(method = "NRC")

kable(head(nrc_Christmas_list), caption = "Word-level analysis -- NRC lexicon")
kable(unique(nrc_Christmas_list$sentiment), col.names = "sentiment", caption = "Sentiment column -- NRC lexicon")
structure(nrc_Christmas_list)
```

According to the tables from 1 to 6, we can tell that:

In afinn lexicon, the value column represents the sentiment of each word
ranged from -5 to 5, where negative values refer to the negative
sentiment and positive values represent the positive sentiment.

In the Bing lexicon, the sentiment column represents the sentiment of
each word with 2 kinds of outputs- "negative" and "positive", which is
easy to tell what they represent respectively.

In the NRC lexicon, the sentiment column also represents the sentiment
of each word with not only "negative" and "positive", but also other
outputs like "fear", "sadness", "anger","anticipation", "disgust",
"joy", "trust", and "surprise", so the sentiment description in NRC
lexicon is much more detailed.

And because of the different numbers of words in lexicons, the outcomes
of inner join different. The results inner joined by nrc lexicon is
larger because there are more words in the nrc lexicon.

In my opinion, the values in afinn lexicon and the sentiment in Bing
lexicon(which can be transformed into dummy variables) are
machine-readable and easy to process. But for the nrc lexicon, for now I
can only take the "positive" and "negative" into account to make it
machine-readable and easy to process because of the lack of assessments
of other sentiments.

```{r}
# 
afinn_Christmas <- afinn_Christmas_list %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  bing_Christmas_list,
  tidy_Christmas %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)






# afinn_Christmas
# df1 <- bing_and_nrc
# df2 <- bind_rows(bing_Chritmas_list,nrc_Chritmas_list)%>%
#   count(method, index = linenumber %/% 80, sentiment) %>%
#   pivot_wider(names_from = sentiment,
#               values_from = n,
#               values_fill = 0) %>% 
#   mutate(sentiment = positive - negative)
  
```

```{r fig.cap="sentiment plot for A Christmas Carol"}
#combine these 3 dataframe with different sentiment analysis methods and make the plot for sentiment analysis

bind_rows(afinn_Christmas, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")+
  theme_bw()
```

Figure 1 is the sentiment progress for every 80 lines in this book with different books. We can tell the sentiment changes through the progression of plotline. And different lexicons show a little
different result. 

From my perspective, the sentiment analysis by Bing lexicon better explains the sentiment of the book as the progression of the plots.

Now I will pick up the Bing lexicon as the main lexicon to do the word count analysis and other analysis.

```{r}
bing_word_counts <- tidy_Christmas %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

```{r fig.width=6, fig.height=2,fig.cap="negative positive words count"}
#
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)+
  theme_bw()
```

Figure 2 presents the top 10 negative and positive word count. In the
negative barplot chart, the word "poor" is the most common word followed
by word "cold" and "dark". In the positive part, The most frequent word
is "good" with "like" and "great" following after.

```{r fig.width=6, fig.height=4,fig.cap='word cloud'}
library(wordcloud)

# set seed to make the word cloud fixed
set.seed("3")
tidy_Christmas %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = "brown2"))
```

Figure 3 displays word cloud where we can get the frequency for top 100
words with the size representing the word count. As is shown in the
figure, "Scrooge", the leading character in this book, is the most
common word by bing lexicon, which makes sense in this case. And the
"Christmas", "ghost" and "spirit" follow after, which represent the
period of time, objects talking about in the main plotline. the It is
reasonable because they are the main characters in that fiction book.
And we can also find "Bob", another main character in this fiction,
which I will do the character analysis along with Scrooge.

```{r fig.width=6, fig.height=4,fig.cap="Word Cloud with Sentiment Analysis"}
#
library(reshape2)

tidy_Christmas %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("brown2", "darkolivegreen3"),
                   max.words = 100, scale = c(4.6,0.2))
```

Figure 4 shows the word frequency of "positive" and "negative" words,
where the size represents the word count. This part is also shown in
Figure 2, but now we show them in a different way.

### Summarization

To summarize, from Figure 1, we can tell the sentiment changes through
the progression of plotline. And different lexicons show a little
different result. But here, as I said at the beginning, I choose Bing
lexicon as the best lexicon to explain.

At the very beginning, the sentiment of the book is negative, which
corresponding to the content of the book which tells the background of
the story and it's a poor situation in Ireland. Later on, the book tells
that Scrooge's living situation and he is rich, and in Figure 1 the
sentiment is positive. And, the sentiment turns to negative for a while,
goes back to positive and this process repeats twice. And that makes
sense for the book because Scrooge experiences something bad at
Christmas Eve and then changes himself into a good man at last.

From Figure 2 and Figure 4, we can capture the top 100 most frequent
sentimental words exist in this book. The most frequent negative one is
"poor" and it can be translated into a poor situation in Ireland in
1840s, which is the background of the story. The most frequent positive
one is "good", which is general in many books and here it can also
represent the personality of Scrooge at last.

In Figure 3 part, I've already explained something below the figure, and
the count of word in this book gives us the information about who is the
leading character--Scrooge.

### Extra Credit

In addition to the 3 lexicons I used, I noticed that there is another
lexicon called "Loughran-McDonald". Now, I am going to use this lexicon
to make some similar plot and give an general idea of the plotline of
the book from the start to the end.

```{r fig.width=6, fig.height=4,fig.cap="Sentiment Analysis by LM lexicon"}
# this is similar to previous code
# use inner join to match the word in the fiction and add the sentiment column
LM_lexicon <-tidy_Christmas %>% 
  inner_join(get_sentiments("loughran")) %>%
  mutate(method = "Loughran-McDonald")  %>% 
  count(method, index = linenumber %/% 80, sentiment) %>% 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# use ggplot to make the barplot for sentiment analysis
LM_lexicon %>%ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +labs(title='Loughran-McDonald')+
  theme_bw()+theme(plot.title = element_text(hjust = 0.5))
```

According to the plot we found that the sentiment in some part are zero, which doesn't happen in other lexicons. In the webpage [https://sraf.nd.edu/textual-analysis/contributed-materials/](https://sraf.nd.edu/textual-analysis/contributed-materials/), I notice that this lexicon is specially for accounting and financial documents. 

Consequently, the conclusion can be drawn that this lexicon is not very suitable for fiction sentiment analysis, which can also tell in Figure 5 because the sentiment does not properly match the plotline in the book.

## task 3 sentence-level analysis

### Tnum

Now, I input my book, A Christmas Carol, into tnum test2 space, the
following tables are part of the book.

```{r, message=FALSE, warning=FALSE}
# input the book into tnum space
# tnBooksFromLines(Christmas_table$text, "Charlse_Dickens/A_Christmas_Carol_4")

# query the first 100 lines of data and save them into dataframe
q1_100<-tnum.query(query="Charlse_Dickens/A_Christmas_Carol_4# has *",max=100) %>% tnum.objectsToDf()

# use kable to display the dataframe
d1_100 <- kable(head(q1_100), caption = "First 100 Query results in tnum")
d1_100
```

```{r echo=TRUE, message=FALSE}
# get the text column
df_heading<- tnum.query('Charlse_Dickens/A_Christmas_Carol_4/heading# has text',max=90) %>% tnum.objectsToDf()
kable(df_heading %>% select(subject:numeric.value),  caption = "Query results with subject heading")
```

```{r echo=TRUE}

df_text <- tnum.query('Charlse_Dickens/A_Christmas_Carol_4# has text',max=60) %>% tnum.objectsToDf()

kable(df_text %>% select(subject:string.value)%>% head(),  caption = "Display the string.value property of the results")


```
Then I use sentimentr to get sentiment score group by these scores with section to get the average result. The plot sort the average sentiment score from high to low.
```{r fig.width=6, fig.height=4,fig.cap="Sentiment Analysis by sentence"}
#query the texts with the subject section and save them into dataframe

df_section<- tnum.query('Charlse_Dickens/A_Christmas_Carol_4/section# has text',max=9000) %>% tnum.objectsToDf()

# split the subject column into several new columns 
Christmas_by_sentence<-df_section %>% separate(col=subject,
                  into = c("path1", "path2","section","paragraph","sentence"), 
                  sep = "/", 
                  fill = "right") %>% 
  select(section:string.value)


#book_sentence$section<-str_extract_all(book_sentence$section,"\\d+") %>% unlist() %>% as.numeric()
Christmas_by_sentence<-Christmas_by_sentence %>% mutate_at(c('section','paragraph','sentence'),~str_extract_all(.,"\\d+") %>% unlist() %>% as.numeric())


### change the name of sentence_out
sentence_out<-Christmas_by_sentence %>% mutate(sentence_split = get_sentences(string.value))%$%
    sentiment_by(sentence_split, list(section))

plot(sentence_out)

```
As we can see in Figure 6, I make the sentiment analysis by sentence and grouped the score of sentiment into 5 chapters. The red points represent the mean score for each chapter.

### Compare this analysis with the analysis you did in Task TWO

We cannot compare the methods of package Sentimentr and Bing lexicon directly because they assess the sentiment in a different scale. At this point, it's good to try some standardization methods. Here I will scale this 2 kinds of scores and show the sentiment analysis grouped by chapters and then we can tell the sentimental progressions in this fiction  with 2 different methods.

In this case there are 3 scale methods I use:
1. standerdization
2. make a rank for these 5 chapter



According to the result, I would like to say the "" method is the best sentiment analysis method for this book.


```{r}
normalize<-function(x){
  re<-(x-min(x)) /(max(x)-min(x))
  return(re)
}

normalize_log<-function(x){
  re<-(log(x) /log(max(x)))
  return(re)
}
```

```{r fig.cap="sentiment comparison- normalize"}
bing_new<-tidy_Christmas %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al.") %>% 
    count(method, index = chapter, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)



bing_new_Chritmas<-bing_new %>% mutate(bing_scale=normalize(sentiment)) %>% select(method,index,bing_scale)
colnames(bing_new_Chritmas)[2]='section'

sentence_out<-sentence_out %>% mutate(sentimentr_scale=normalize(ave_sentiment))


sentence_out_2method<-left_join(sentence_out,bing_new_Chritmas,by='section')%>% select(section,bing_scale,sentimentr_scale)
sentence_out_2method_plot<-sentence_out_2method %>% pivot_longer(cols=c('sentimentr_scale','bing_scale'),names_to = 'sentiment')

sentence_out_2method_plot %>%ggplot(aes(y=value,x=factor(section))) +
  geom_bar(aes(fill=sentiment),stat='identity',position = "dodge",width = 0.7)+theme_bw()

```
In Figure 7, when we use the normalization to process the scores, we can see there is 0 for both methods in Chapter 1, but for the latter chapters, using bing lexicon shows a more extreme sentiment (whether more positive or more negative), which seems to be more reasonable. 

```{r fig.cap="sentiment comparison- rank"}
bing_new<-tidy_Christmas %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al.") %>% 
    count(method, index = chapter, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)



bing_new_Chritmas<-bing_new %>% mutate(bing_scale=rank(sentiment)) %>% select(method,index,bing_scale)
colnames(bing_new_Chritmas)[2]='section'

sentence_out<-sentence_out %>% mutate(sentimentr_scale=rank(ave_sentiment))


sentence_out_2method<-left_join(sentence_out,bing_new_Chritmas,by='section')%>% select(section,bing_scale,sentimentr_scale)
sentence_out_2method_plot<-sentence_out_2method %>% pivot_longer(cols=c('sentimentr_scale','bing_scale'),names_to = 'sentiment')

sentence_out_2method_plot %>%ggplot(aes(y=value,x=factor(section))) +
  geom_bar(aes(fill=sentiment),stat='identity',position = "dodge",width = 0.7)+theme_bw()

```
In Figure 8, the larger the rank is, the more positive the sentiment is. So we can tell that there is a slight different in Chapter 2 and Chapter 3 in diferent methods and I think it's acceptable for both methods as is shown in this figure.

\newpage

### EXTRA CREDIT: Character Analysis

In this book, as we showed in the word cloud, Scrooge and Bob are 2 leading
characters. Now, I prefer to do the character analysis for them:

First, I am going to calculate the frequency for the characters.

The following table in the count number of times each character appears
in each chapter:

```{r eval=FALSE}
#theme(legend.key.size = unit(2, 'cm'),legend.title = element_text(size=30),legend.text = element_text(size=30))
book_sentence_indi<-Christmas_by_sentence %>% mutate(scrooge=str_match(Christmas_by_sentence$string.value,regex('([Ss]crooge)'))[,1],
                         bob=str_match(Christmas_by_sentence$string.value,regex('([Bb]ob)'))[,1])


score<-book_sentence_indi %>% dplyr::mutate(sentence_split = get_sentences(string.value))%$%
    sentiment_by(sentence_split) %>% `$`(ave_sentiment)

book_sentence_indi$score<-score
re<-book_sentence_indi %>% group_by(section) %>% summarise(scrooge=sum(scrooge %>% is.na() %>% `!`()),
                                                       bob=sum(bob%>% is.na() %>% `!`()))

#knitr::kable(re,'simple')
re2<-book_sentence_indi %>% group_by(section,paragraph) %>% summarise(
  both_appear=sum(scrooge %>% is.na() %>% `!`() & bob%>% is.na() %>% `!`() ))

#re2 %>% filter(both_appear>0)
kable(re,'simple', caption = "The Count for each Character")

kable(re2 %>% filter(both_appear>0),'simple', caption = "The Count when Both Characters appear")





```

```{r}
# tnum.getDBPathList(taxonomy="subject", levels=2)
# tnum.graphPathList(pathList = ,rootLabel = "")
```

## Reference



1. [A Christmas Carol in Prose; Being a Ghost Story of Christmas by Charles Dickens](https://www.gutenberg.org/ebooks/46)

2. [Software Repository for Account and Finance](https://sraf.nd.edu/textual-analysis/contributed-materials/)

3. [Text Mining with R](https://www.tidytextmining.com/sentiment.html)

4. The ideas and supports from my dear classmate Yuli Jin.
